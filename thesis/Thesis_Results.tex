%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                      %
%     File: Thesis_Results.tex                                         %
%     Tex Master: Thesis.tex                                           %
%                                                                      %
%     Author: Andre C. Marta                                           %
%     Last modified :  2 Jul 2015                                      %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Evaluation}
\label{chapter:results}

This chapter presents the results of our taint analysis. We discuss the ability of \toolname{} to detect vulnerabilities and the effort needed to add support for new languages. Finally, we talk about some limitations that our implementation has.

\section{Experimental Evaluation}
\label{evaluation}
The objective of this section is to show that \toolname{} is capable of finding vulnerabilities in web applications written in different languages and that the effort needed to add a new language to the tool is relatively small. First, we present the results of analyzing several web applications in \implangs{}. Then, we discuss the effort needed to add support for another language.


\subsection{Vulnerability detection}
In order to show the ability of \toolname{} to analyze and find vulnerabilities in web applications, we tested \toolname{} against two types of web applications. First, we chose 11 open source applications from GitHub that are deliberately insecure, with documented vulnerabilities. The criteria used to choose them was the number of stars that each application has on GitHub, essentially choosing the most known ones. To run the tests, we had to manually identify the entry points and sensitive functions for each application, meaning that \toolname{} analyzed each application several times, once for each entry point. Second, we also tested \toolname{} against two real-world open-source web applications. Since these applications are much bigger, we assumed that every file from the application is a web page that can receive user input, tainting the variables that might be influenced by the user (e.g., \$\_GET[*] in PHP). Tables \ref{results} and \ref{results1} show the results of our analysis. In the data, we only include files with the extension that we analyzed (e.g., *.java and *.php). Furthermore, we excluded comments and blank lines from the line count. We ran the tests on a computer with a Ryzen 1600 processor (6 cores, 12 threads at 3.6GHz) and 16GB of RAM. 

\begin{table*}[htbp]
    \caption{Deliberately insecure web applications}
    \begin{center}
        \begin{tabular}{|l|| l | l  |l | l | l | l |}
            \hline
            \textbf{Application}  & \textbf{\#loc}      & \textbf{Language}      & \textbf{Files}     & \textbf{Vuln. Found } & \textbf{False pos.} & \textbf{False neg.} \\ [0.5ex] 
            \hline\hline   
            WebGoat 8             & 13898     & Java       & 320        & 11 & 1 & 0\\
            \hline
            Vulnado               & 423       & Java       & 11         & 3 & 0 & 0\\
            \hline
            Dvja                  & 950       & Java       & 21         & 4 & 0 & 0\\
            \hline
             DVWA                 & 19651     & PHP        & 358        & 18 & 3  & 0\\
            \hline
             OWASP Vwa            & 1018      & PHP        & 27         & 17 & 1 & 0\\   
            \hline
             Vulnerable-node      & 4207      & JavaScript & 13         & 5 & 0 & 0\\  
            \hline
             Dvna                 & 771      & JavaScript  & 14         & 0 & 0 & 4\\
            \hline
             Goof                 & 571      & JavaScript  & 8         & 3 & 0 & 0 \\
            \hline  
             NodeGoat                 & 2697      & JavaScript  & 49   & 6 & 0& 0 \\
            \hline  
             Vulpy                 & 2373      & Python    & 57         & 6  & 0& 0\\
            \hline  
             Dvpwa                 & 674      & Python    & 21         & 7  & 0 & 0\\   [0.5ex]  
            \hline\hline   
            \textbf{Total}        & 47233 &              &  899 &  78  & 5 &  4\\
            \hline
        \end{tabular}
    \label{results}
    \end{center}
\end{table*} 


\begin{table*}[htbp]
    \caption{Real-world web applications}
    \begin{center}
        \begin{tabular}{|l|| l | l  |l | l |}
            \hline
            \textbf{Application}  & \textbf{\#loc}      & \textbf{Language}      & \textbf{Files}     & \textbf{Vuln. found} \\ [0.5ex] 
            \hline\hline   
             SquirrelMail 1.5     & 46214     & PHP        & 376         & 0 \\ 
            \hline
             PhpMyAdmin 4.9.5     & 153576    & PHP        & 740        & 1 \\  [0.5ex]    
            \hline\hline   
            \textbf{Total}        & 199790 &              &  1116 &  1 \\
            \hline
        \end{tabular}
    \label{results1}
    \end{center}
\end{table*}


\toolname{} analyzed 2015 files and 247023 lines of code and managed to find 79 documented vulnerabilities, such as SQL injection, cross-site scripting, command injection and file inclusion. The analysis times were quite low. The application that took the longest to analyze was \textit{PhpMyAdmin} with 82 seconds. However, the analysis times varied a lot depending on the entry point, meaning that the longer the path through which the data flows, the longer the analysis time.

In our tests, the tool had 4 false negatives in Dvna because all the dangerous code is in anonymous functions that are called by referencing variables, since we do not track the value of each variable, this is a limitation. Also, we had 1 false negative when testing PhpMyAdmin.

\toolname{} only raised 5 false positives in the applications from table \ref{results}, due to data flow propagation through impossible paths. In total we had 12.7\% of false positives and negatives. We may assume that the low rate is due to our conservative taint analysis and to the fact that most of the deliberately insecure applications we have tested do not have very complex control flows.



\subsection{Portability}

Since the objective of this work is to support several languages with as little effort as possible, the portability of the tool is also a metric that we tested. To test the portability, we first implemented the tool to support PHP analysis, and then we added support for Java, which is a substantially different language. While adding support for PHP and Java we were also developing the other modules, so it is hard to tell how much time was spent strictly adding support for each language. However, later, after the tool was built, we added support for another two very popular languages: JavaScript and Python. Adding support for each of them took us roughly 7 hours. Table \ref{results} shows that our implementations are able to find vulnerabilities. Table \ref{results} shows that our implementations are able to find vulnerabilities. In our opinion, the main challenge when adding support for a new language is identifying which elements from the grammar are important to the analysis. After that, we just have to  write the converter and some unit tests to make sure the converter works properly.

Table \ref{converters} presents the order in which the languages were added, the number of lines of each converter, the number of unique statements and the number of hours spent developing each one of them.
The number of unique statements is meant to show that the converter does not have much logic, it basically consists of calls to the \astbuilder{}. For example, the converter for Java has 108 lines, 30 of which are repeated (e.g., \textit{exitStatementOrExpression()} is invoked 19 times), leaving us with 78 unique statements.

\begin{table}[htbp!]
    \caption{Converters size and implementation effort}
    \begin{center}
        \begin{tabular}{|c|l|l|l|l|}
           \hline
           \thead{Implementation order} & \thead{Language} & \thead{\#loc} & \thead{Unique statements} & \thead{Hours to \\ implement} \\ [0.5ex] 
        
            \hline\hline
            1 &  PHP & 67 & 49 & --\\

            \hline
            2 & Java & 108 & 78 & --\\
            
            \hline
            3 & JavaScript & 50 & 41 & 7 \\

            \hline
            4 &  Python & 61 & 49 & 7 \\
           \hline
          \end{tabular}
          \label{converters}
    \end{center}
    
\end{table}



\section{Limitations}
\label{limitations}
In this section we present the limitations of our tool. Most of them are related to the nature of static taint analysis, such as propagating the data flow through impossible paths or the pointer analysis problem. Besides the limitations already discussed we also found another related to lambda functions. Since our taint analysis only propagates \textit{taint} marks, we can not track the implementation of lambda functions.
For example, consider listing \ref{lambda}, written in JavaScript, and assume that the \textit{userInput} is tainted. In this case, we have an arrow function (lambda equivalent in JavaScript) \textit{x} that accepts a parameter \textit{query} and then executes this query against the database. This is a very simple program, with only two lines, however, \toolname{} struggles to find the vulnerability from line 3. This flaw is due to the fact that in our implementation, \textit{x} is just a variable, it does not know that \textit{x} is an object that holds a function. Furthermore, this variable \textit{x} could be passed around as an argument to other functions. So, in complex cases, we quickly lose track of \textit{x} and are unable to know what is the implementation of the function that \textit{x} is holding.


\begin{lstlisting}[
    showstringspaces=false,
    caption={JavaScript lambda example},
    label=lambda]
var x = query => db.executeQuery(query);

x.call("SELECT * FROM users WHERE name=" + userInput);
\end{lstlisting}

This problem does not have an easy solution since it would require the tool to precisely track the flow of data in the application, which statically is virtually impossible.